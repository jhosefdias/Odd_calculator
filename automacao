#!/usr/bin/env python3
"""
Agente aut√¥nomo para monitorar partidas, atualizar estat√≠sticas e odds,
gerar an√°lises com Gemini e enviar notifica√ß√µes.

Principais responsabilidades:
1. Consultar periodicamente a API Football-Data e identificar partidas novas.
2. Atualizar incrementos em arquivos CSV sem recalcular o hist√≥rico inteiro.
3. Calcular probabilidades/odds apenas para os times afetados.
4. Gerar an√°lises em lote via Gemini e anexar ao CSV final.
5. Registrar logs estruturados e enviar resumo opcional via Discord.
"""

from __future__ import annotations

import argparse
import asyncio
import logging
import os
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Set

import pandas as pd
import requests
from dotenv import load_dotenv
from importlib.machinery import SourceFileLoader

# Aproveita fun√ß√µes existentes para garantir consist√™ncia dos c√°lculos.
from analisador_odds import calcular_probabilidades, gerar_analises_em_lote

BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data"
DATA_DIR.mkdir(exist_ok=True)

MATCH_HISTORY_PATH = DATA_DIR / "matches_history.csv"
AGGREGATED_PATH = BASE_DIR / "brasileirao_2025_parcial_atualizado.csv"
AGGREGATED_FALLBACK = BASE_DIR / "brasileirao_2025_parcial.csv"
ODDS_PATH = BASE_DIR / "brasileirao_2025_odds.csv"

API_BASE_URL = "https://api.football-data.org/v4/matches"

LOGS_DIR = BASE_DIR / "logs"
LOGS_DIR.mkdir(exist_ok=True)


@dataclass
class AgentConfig:
    api_token: str
    competition: str = "BSA"
    fetch_interval: int = 900  # 15 minutos
    discord_webhook: Optional[str] = None


@dataclass
class AgentResult:
    fetched: int = 0
    new_matches: int = 0
    teams_updated: Set[str] = None
    odds_updated: int = 0
    skipped_reason: Optional[str] = None

    def __post_init__(self) -> None:
        if self.teams_updated is None:
            self.teams_updated = set()



def setup_logging(level: int = logging.INFO) -> None:
    formatter = logging.Formatter(
        "%(asctime)s | %(levelname)s | %(message)s", "%Y-%m-%d %H:%M:%S"
    )

    root_logger = logging.getLogger()
    root_logger.setLevel(level)

    file_handler = logging.FileHandler(LOGS_DIR / "automation.log")
    file_handler.setFormatter(formatter)

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)

    root_logger.handlers.clear()
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)


def send_discord_notification(webhook_url: str, message: str) -> None:
    try:
        response = requests.post(
            webhook_url,
            json={"content": message},
            timeout=15,
        )
        response.raise_for_status()
        logging.info("Notifica√ß√£o enviada ao Discord.")
    except Exception as exc:  # noqa: BLE001
        logging.warning("Falha ao enviar notifica√ß√£o ao Discord: %s", exc)


def load_interface_module() -> object:
    """Carrega o m√≥dulo da interface para reutilizar exibi√ß√£o se necess√°rio."""
    interface_path = BASE_DIR / "Terminal" / "Interface"
    if not interface_path.exists():
        raise FileNotFoundError(f"Arquivo Interface n√£o encontrado: {interface_path}")

    loader = SourceFileLoader("odds_interface", str(interface_path))
    module = loader.load_module()  # type: ignore[deprecated-return-value]
    return module

#agente

class OddsAutomationAgent:
    def __init__(self, config: AgentConfig) -> None:
        self.config = config
        self.session = requests.Session()
        self.session.headers.update({"X-Auth-Token": config.api_token})
        self.last_fetch: Optional[datetime] = None

    # ------------------------------ API ---------------------------------- #
    def fetch_matches(self) -> pd.DataFrame:
        params = {
            "competitions": self.config.competition,
            "status": "FINISHED",
        }

        if MATCH_HISTORY_PATH.exists():
            history = pd.read_csv(MATCH_HISTORY_PATH)
            if not history.empty and "utc_date" in history.columns:
                last_date = pd.to_datetime(history["utc_date"]).max()
                if pd.notnull(last_date):
                    params["dateFrom"] = last_date.strftime("%Y-%m-%d")

        logging.debug("Consultando API Football-Data com params: %s", params)

        response = self.session.get(API_BASE_URL, params=params, timeout=30)
        response.raise_for_status()

        payload = response.json()
        matches = payload.get("matches", [])
        logging.info("Consulta retornou %d partidas.", len(matches))

        data_rows = []
        for item in matches:
            try:
                if item.get("status") != "FINISHED":
                    continue
                data_rows.append(
                    {
                        "match_id": item["id"],
                        "utc_date": item["utcDate"],
                        "home_team": item["homeTeam"]["name"],
                        "away_team": item["awayTeam"]["name"],
                        "home_score": item["score"]["fullTime"]["home"],
                        "away_score": item["score"]["fullTime"]["away"],
                        "winner": item["score"].get("winner"),
                        "competition": item.get("competition", {}).get("name", ""),
                    }
                )
            except KeyError:
                logging.debug("Partida ignorada por dados incompletos: %s", item)

        return pd.DataFrame(data_rows)

    

    def load_match_history(self) -> pd.DataFrame:
        if MATCH_HISTORY_PATH.exists():
            return pd.read_csv(MATCH_HISTORY_PATH)
        return pd.DataFrame(
            columns=[
                "match_id",
                "utc_date",
                "home_team",
                "away_team",
                "home_score",
                "away_score",
                "winner",
                "competition",
            ]
        )

    def persist_match_history(self, history_df: pd.DataFrame) -> None:
        history_df.to_csv(MATCH_HISTORY_PATH, index=False, encoding="utf-8-sig")
        logging.info("Hist√≥rico de partidas atualizado (%s).", MATCH_HISTORY_PATH)

    # ---------------------- Atualiza√ß√£o incremental ----------------------- #

    @staticmethod
    def _ensure_team_row(df: pd.DataFrame, team: str) -> None:
        if team not in df.index:
            df.loc[team] = {
                "gols_marcados": 0,
                "gols_sofridos": 0,
                "cart√µes": 0,
                "vit√≥rias": 0,
                "empates": 0,
                "derrotas": 0,
                "partidas": 0,
            }

    def _load_aggregated_table(self) -> pd.DataFrame:
        if AGGREGATED_PATH.exists():
            df = pd.read_csv(AGGREGATED_PATH)
        elif AGGREGATED_FALLBACK.exists():
            df = pd.read_csv(AGGREGATED_FALLBACK)
        else:
            df = pd.DataFrame(
                columns=[
                    "Time",
                    "gols_marcados",
                    "gols_sofridos",
                    "cart√µes",
                    "vit√≥rias",
                    "empates",
                    "derrotas",
                    "partidas",
                ]
            )
        df = df.set_index("Time")
        return df

    def _persist_aggregated_table(self, df: pd.DataFrame) -> None:
        df.sort_index().to_csv(AGGREGATED_PATH, encoding="utf-8-sig")
        logging.info("Tabela agregada atualizada (%s).", AGGREGATED_PATH)

    def _update_team_stats(
        self,
        df: pd.DataFrame,
        row: pd.Series,
        home: bool,
    ) -> None:
        team = row["home_team"] if home else row["away_team"]
        opponent_score = row["away_score"] if home else row["home_score"]
        own_score = row["home_score"] if home else row["away_score"]

        self._ensure_team_row(df, team)

        df.at[team, "gols_marcados"] += own_score or 0
        df.at[team, "gols_sofridos"] += opponent_score or 0
        df.at[team, "partidas"] += 1

        result = None
        if row["winner"] == "HOME_TEAM":
            result = "vit√≥rias" if home else "derrotas"
        elif row["winner"] == "AWAY_TEAM":
            result = "vit√≥rias" if not home else "derrotas"
        else:
            result = "empates"

        df.at[team, result] += 1

    def update_aggregated_stats(self, new_matches: pd.DataFrame) -> Set[str]:
        if new_matches.empty:
            return set()

        agg_df = self._load_aggregated_table()
        teams_touched: Set[str] = set()

        for _, row in new_matches.iterrows():
            self._update_team_stats(agg_df, row, home=True)
            self._update_team_stats(agg_df, row, home=False)
            teams_touched.add(row["home_team"])
            teams_touched.add(row["away_team"])

        agg_df = agg_df.astype(
            {
                "gols_marcados": "int64",
                "gols_sofridos": "int64",
                "cart√µes": "int64",
                "vit√≥rias": "int64",
                "empates": "int64",
                "derrotas": "int64",
                "partidas": "int64",
            },
            copy=False,
        )

        self._persist_aggregated_table(agg_df)
        return teams_touched

    # --------------------------- Odds ------------------------------------- #

    def update_odds(self, teams: Set[str]) -> int:
        if not teams:
            logging.info("Nenhum time teve partidas novas; odds preservadas.")
            return 0

        agg_df = pd.read_csv(AGGREGATED_PATH)
        subset = agg_df[agg_df["Time"].isin(teams)].copy()
        if subset.empty:
            logging.info("Nenhum registro agregado encontrado para atualiza√ß√£o.")
            return 0

        subset = calcular_probabilidades(subset)
        subset = gerar_analises_em_lote(subset)

        if ODDS_PATH.exists():
            odds_df = pd.read_csv(ODDS_PATH)
            odds_df = odds_df[~odds_df["Time"].isin(teams)]
            combined = pd.concat([odds_df, subset], ignore_index=True)
        else:
            combined = subset

        combined.sort_values(by="Time", inplace=True)
        combined.to_csv(ODDS_PATH, index=False, encoding="utf-8-sig")

        logging.info(
            "Odds atualizadas para %d time(s) e salvas em %s.",
            len(teams),
            ODDS_PATH,
        )
        return len(teams)

    # ------------------------- Execu√ß√£o principal ------------------------- #

    def detect_new_matches(
        self, history: pd.DataFrame, fresh: pd.DataFrame
    ) -> pd.DataFrame:
        if history.empty:
            return fresh
        already = set(history["match_id"].astype(str))
        fresh["match_id"] = fresh["match_id"].astype(str)
        return fresh[~fresh["match_id"].isin(already)]

    def run_once(self) -> AgentResult:
        result = AgentResult()
        try:
            fresh_matches = self.fetch_matches()
            result.fetched = len(fresh_matches)
        except Exception as exc:  # noqa: BLE001
            logging.error("Erro ao consultar API: %s", exc)
            result.skipped_reason = f"API error: {exc}"
            return result

        history = self.load_match_history()
        new_matches = self.detect_new_matches(history, fresh_matches)
        result.new_matches = len(new_matches)

        if new_matches.empty:
            logging.info("Nenhuma partida nova encontrada. Aguardando pr√≥ximo ciclo.")
            result.skipped_reason = "Nenhuma partida nova"
            return result

        updated_history = pd.concat([history, new_matches], ignore_index=True)
        self.persist_match_history(updated_history)

        teams = self.update_aggregated_stats(new_matches)
        result.teams_updated = teams

        updated_count = self.update_odds(teams)
        result.odds_updated = updated_count

        logging.info(
            "Execu√ß√£o conclu√≠da: %d partidas novas, %d time(s) atualizados.",
            result.new_matches,
            result.odds_updated,
        )
        return result

    async def run_forever(self) -> None:
        while True:
            start = datetime.now(timezone.utc)
            logging.info("Iniciando ciclo de automa√ß√£o √†s %s.", start.isoformat())

            result = self.run_once()

            if self.config.discord_webhook:
                summary = self._build_summary_message(result, start)
                send_discord_notification(self.config.discord_webhook, summary)

            elapsed = datetime.now(timezone.utc) - start
            sleep_for = max(self.config.fetch_interval - int(elapsed.total_seconds()), 5)
            logging.info("Pr√≥xima verifica√ß√£o em %d segundos.", sleep_for)
            await asyncio.sleep(sleep_for)

    @staticmethod
    def _build_summary_message(result: AgentResult, start: datetime) -> str:
        timestamp = start.strftime("%Y-%m-%d %H:%M UTC")
        if result.skipped_reason:
            return (
                f"üü° Automa√ß√£o de odds ({timestamp})\n"
                f"- Partidas consultadas: {result.fetched}\n"
                f"- Atualiza√ß√£o ignorada: {result.skipped_reason}"
            )
        return (
            f"‚úÖ Automa√ß√£o de odds ({timestamp})\n"
            f"- Partidas novas: {result.new_matches}\n"
            f"- Times atualizados: {', '.join(sorted(result.teams_updated)) or 'nenhum'}\n"
            f"- Odds recalculadas: {result.odds_updated}"
        )



def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Agente aut√¥nomo para atualiza√ß√£o de odds do Brasileir√£o."
    )
    parser.add_argument(
        "--once",
        action="store_true",
        help="Executa apenas um ciclo de atualiza√ß√£o.",
    )
    parser.add_argument(
        "--interval",
        type=int,
        default=None,
        help="Intervalo em segundos entre verifica√ß√µes (padr√£o: 900).",
    )
    parser.add_argument(
        "--competition",
        type=str,
        default=None,
        help="C√≥digo da competi√ß√£o na API Football-Data (padr√£o: BSA).",
    )
    parser.add_argument(
        "--log-level",
        type=str,
        default="INFO",
        help="N√≠vel de log (DEBUG, INFO, WARNING, ERROR).",
    )
    return parser.parse_args()


def build_config(args: argparse.Namespace) -> AgentConfig:
    load_dotenv()

    api_token = os.getenv("API_KEY")
    if not api_token:
        raise EnvironmentError("API_KEY n√£o configurada no .env.")

    discord_webhook = os.getenv("DISCORD_WEBHOOK_URL")

    config = AgentConfig(
        api_token=api_token,
        competition=args.competition or os.getenv("COMPETITION_CODE", "BSA"),
        fetch_interval=args.interval or int(os.getenv("FETCH_INTERVAL", 900)),
        discord_webhook=discord_webhook,
    )
    return config


def main() -> None:
    args = parse_args()
    setup_logging(getattr(logging, args.log_level.upper(), logging.INFO))

    try:
        config = build_config(args)
    except Exception as exc:  # noqa: BLE001
        logging.error("Falha na configura√ß√£o do agente: %s", exc)
        raise SystemExit(1) from exc

    agent = OddsAutomationAgent(config)

    if args.once:
        result = agent.run_once()
        logging.info("Resultado final: %s", result)
    else:
        try:
            asyncio.run(agent.run_forever())
        except KeyboardInterrupt:
            logging.info("Execu√ß√£o interrompida manualmente.")


if __name__ == "__main__":
    main()
